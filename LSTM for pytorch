'''
    简单的LSTM网络用于预测的模型搭建
     created by chains
'''

import torch
from torch import nn
import numpy as np
import matplotlib.pyplot as plt
import torch.utils.data as Data
import pandas as pd
from torch.autograd import Variable
from sklearn.metrics import mean_squared_error  #均方误差
from sklearn.metrics import mean_absolute_error #平方绝对误差
from sklearn.metrics import r2_score            #R square
from sklearn.preprocessing import MinMaxScaler, StandardScaler

#参变量的定义
train_days = 2000      #训练集所用的天数
testing_days = 394     #测试集所用天数

# 导入数据集
df = pd.read_table("D:\\科研相关\\debutanizer_data.txt", sep='\s+', header=None)     #读取txt文件的数字部分，无表头[2394,8]
data = df.values                                                                    #将其转换为numpy数组，大小为[2394,8]
data = data.astype(np.float32)                                                      #size[2394,8]

#划分数据集测试集
train_data = data[:train_days, :]                    #size[2000,8]
test_data  = data[train_days:2300, :]                    #size[394,8]
#print(train_data.shape,test_data.shape)

#数据预处理
std = StandardScaler()##
train_data = std.fit_transform(train_data)           #size[2000,8]
test_data  = std.transform(test_data)                #size[2000,8]

#Parameter definition
Input_size = 7                #输入变量的维度
Hidden_size = 300             #隐藏变量的维度
Batch_size = 50               #批训练大小
num_epochs = 300              #训练次数
Num_layers = 1                #堆叠LSTM的层数
LR = 0.0015                  #训练时的学习率

class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, bias=True):
        super(RNN, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.simpleLSTM = nn.LSTM(input_size, hidden_size, num_layers=self.num_layers, bias = bias)
        self.out = nn.Linear(hidden_size, 1)

    def forward(self, x):
        h = Variable(torch.zeros(self.num_layers, x.size(1), self.hidden_size))
        s = Variable(torch.zeros(self.num_layers, x.size(1), self.hidden_size))
        out, s_out = self.simpleLSTM(x, (h, s))
        output = self.out(out)
        return output


model = RNN(Input_size, Hidden_size, Num_layers)
optimizer = torch.optim.Adam(model.parameters(), lr=0.00015)
loss_fun = nn.MSELoss()

#训练数据集的加载
train_x = train_data[:, :Input_size]                                 #size[2000,7]
train_y = train_data[:, Input_size].reshape([train_days, 1])         #size[2000,1]
#print(train_x.shape, train_y.shape)
train_x = torch.from_numpy(train_x)                                  #transform to tensor
train_y = torch.from_numpy(train_y)                                  #transform to tensor

torch_dataset = Data.TensorDataset(train_x, train_y)                 #先将数据整理成一个DataSet类
loader = Data.DataLoader(
    dataset=torch_dataset,
    batch_size=Batch_size,
    shuffle=False,
    num_workers=2
)

#trainning step
if __name__ == '__main__':
    Los = []
    for epoch in range(num_epochs):
        loss_sum = 0
        for i, dataset in enumerate(loader):
            inputs, labels = dataset              #size:[500,7],[500,1]
            #print(inputs.shape, labels.shape)
            inputs = inputs.view(-1, Batch_size, Input_size)   #size[1,500,7]
            #print(inputs.shape)
            out = model(inputs)                                #size[1,500,1]
            out = out.view(Batch_size,1)
            #print(out.shape)
            loss = loss_fun(out, labels)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            loss_sum += loss.item()
            print("epoch:", epoch, "|batch:", i, "|loss:", loss)

        Los.append(loss_sum)

    plt.figure()
    plt.plot(range(len(Los)), Los, color='b')
    plt.show()

    #test
    h = Variable(torch.randn(Num_layers, Batch_size, Hidden_size))
    s = Variable(torch.randn(Num_layers, Batch_size, Hidden_size))

    for step, batch in enumerate(loader):
        batch_x, batch_y = batch
        batch_x = batch_x.view(-1, Batch_size, Input_size)
        outs = model(batch_x)
        if step == 0:
            y_pre_train = outs.view(Batch_size)
        else:
            y_pre_train = torch.cat((y_pre_train, outs.view(Batch_size)), dim=0)


    #load test dataset
    test_x = test_data[:, :Input_size]
    test_y = test_data[:, Input_size].reshape([-1, 1])
    test_x = torch.from_numpy(test_x)  # transform to tensor
    test_y = torch.from_numpy(test_y)  # transform to tensor

    torch_test_dataset = Data.TensorDataset(test_x, test_y)  # 先将数据整理成一个DataSet类
    loader_test = Data.DataLoader(
        dataset=torch_test_dataset,
        batch_size=Batch_size,
        shuffle=False,
        num_workers=2
    )

    for steps, batch_test in enumerate(loader_test):
        batch_x_test, batch_y_test = batch_test
        batch_x_test = batch_x_test.view(-1, Batch_size, Input_size)
        y_pred = model(batch_x_test)
        if steps == 0:
            y_pre = y_pred.view(Batch_size)
        else:
            y_pre = torch.cat((y_pre, y_pred.view(Batch_size)), dim=0)

    train_rmse = np.sqrt(mean_squared_error(y_pre_train.data.numpy(), train_data[:len(y_pre_train), Input_size]))
    test_rmse = np.sqrt(mean_squared_error(y_pre.data.numpy(), test_data[:len(y_pre), Input_size]))
    train_r2 = r2_score(y_pre_train.data.numpy(), train_data[:len(y_pre_train), Input_size])
    r2 = r2_score(y_pre.data.numpy(), test_data[:len(y_pre), Input_size])
    print('train_rmse = ' + str(round(train_rmse, 5)))
    print('test_rmse = ' + str(round(test_rmse, 5)))
    print('train_r2 = ', str(train_r2))
    print('r2 = ', str(r2))

    #plotting
    plt.figure()
    plt.plot(train_data[:len(y_pre_train), Input_size], 'b-', label='y_true')
    plt.plot(y_pre_train.data.numpy(), 'r-', label='y_trainpre')
    plt.legend()
    plt.show()

    plt.figure()
    plt.plot(test_data[:len(y_pre), Input_size], 'b-', label='y_true')
    plt.plot(y_pre.data.numpy(), 'r-', label='y_pre')
    plt.legend()
    plt.show()













